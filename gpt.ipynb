{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f0f96ea-8a8a-4adb-91ee-a7ebe18af4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "#import maritalk\n",
    "import openai\n",
    "import pyconll\n",
    "import json\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b282effc-b7cc-450d-b055-49a32e1674d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gptkey.txt') as f:\n",
    "    gpt_key = f.readlines()[0]\n",
    "openai.api_key = gpt_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "566b7338-cec3-451d-9a1d-837f4f661337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askMaritaca(prompt):\n",
    "    model = maritalk.MariTalk(key=maritaca_key)\n",
    "    answer = model.generate(\n",
    "        prompt,\n",
    "        chat_mode=False,\n",
    "        do_sample=True,\n",
    "        #max_tokens=200,\n",
    "        #temperature=0.7,\n",
    "        #top_p=0.85,\n",
    "        #stopping_tokens=[\"\\n\"]\n",
    "    )\n",
    "    #print(f\"Resposta: {answer}\")\n",
    "    time.sleep(6)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4dfea8-5bfa-4fb4-85f1-8893e3a96990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askGPT(prompt, temperature=0.7):\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0dce867-6063-4e2c-822f-2007631f5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def askLLM(prompt, temperature=0.7):\n",
    "    promptReq = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    answer = model.generate(\n",
    "        prompt,\n",
    "        chat_mode=False,\n",
    "        do_sample=True,\n",
    "        #max_tokens=200,\n",
    "        temperature=temperature,\n",
    "        #top_p=0.85,\n",
    "        stopping_tokens=[\"\\n\"]\n",
    "    )    \n",
    "    #resp = response.json()\n",
    "    #predicted_text = resp['responses'][0]['generation']\n",
    "    #endOfAnswer = predicted_text.find('\\n')\n",
    "    time.sleep(6)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18aa7ed7-6050-48cf-9ee8-0a765a71a85f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maskGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuanto Ã© 25 + 27?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m, in \u001b[0;36maskGPT\u001b[0;34m(prompt, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maskGPT\u001b[39m(prompt, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):\n\u001b[1;32m      3\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/venvs/propor/lib/python3.9/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/venvs/propor/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/venvs/propor/lib/python3.9/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/venvs/propor/lib/python3.9/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/propor/lib/python3.9/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "askGPT(\"Quanto Ã© 25 + 27?\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ed2ea-b4e2-40c8-b4b8-bf7b90fca22b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966b6f31-3a7b-422c-b14d-227a99c91870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: 25 + 27 = 52\n"
     ]
    }
   ],
   "source": [
    "model = maritalk.MariTalk(key=maritaca_key)\n",
    "answer = model.generate(\"Quanto Ã© 25 + 27?\")\n",
    "print(f\"Resposta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740cc0f-efd8-4799-8adb-f6adc84d0e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4b1a8-04ad-4415-b3ff-4e46f411ec86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b298ad2-f647-469f-916e-0be50bddefb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading first 1020 dataset sentences\n",
    "frases = pyconll.load_from_file('data/porttinari-base/Porttinari-base_test.conllu')\n",
    "reviews = []\n",
    "for sent in frases[:1020]:\n",
    "    lista_ud=[]\n",
    "    for token in sent:\n",
    "        lista_ud.append([token.form, token.xpos, token.upos, token.deprel])\n",
    "    reviews.append(lista_ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea90ffe5-c3e6-4ffa-9535-9555be3da4b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc5475e6-5057-45dd-a13a-32f799a3da8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atuando como linguista e sem efetuar correÃ§Ãµes ou alteraÃ§Ãµes no texto, faÃ§a a anÃ¡lise morfossintÃ¡tica das frases seguindo a anotaÃ§Ã£o UD (Universal Dependencies):\n",
      "\n",
      "Entrada: O CapitÃ£o AmÃ©rica tambÃ©m bajulou o tucano .\n",
      "Saida: O/DET CapitÃ£o/PROPN AmÃ©rica/PROPN tambÃ©m/ADV bajulou/VERB o/DET tucano/NOUN ./PUNCT\n",
      "\n",
      "Entrada: A Odebrecht pagou 300 % a mais pelo por o direito de explorar o aeroporto do de o GaleÃ£o .\n",
      "Saida: A/DET Odebrecht/PROPN pagou/VERB 300/NUM %/SYM a/ADP mais/ADV pelo/None por/ADP o/DET direito/NOUN de/ADP explorar/VERB o/DET aeroporto/NOUN do/None de/ADP o/DET GaleÃ£o/PROPN ./PUNCT\n",
      "\n",
      "Entrada: No Em o comeÃ§o do de o sÃ©culo , a JBS/Friboi chegava ao a o grupo das de as 400 maiores .\n",
      "Saida: No/None Em/ADP o/DET comeÃ§o/NOUN do/None de/ADP o/DET sÃ©culo/NOUN ,/PUNCT a/DET JBS/Friboi/PROPN chegava/VERB ao/None a/ADP o/DET grupo/NOUN das/None de/ADP as/DET 400/NUM maiores/ADJ ./PUNCT\n",
      "\n",
      "Entrada: Os sons indesejÃ¡veis emitidos por uma porta , por exemplo , sÃ£o eliminados por R$ 150 .\n",
      "Saida: Os/DET sons/NOUN indesejÃ¡veis/ADJ emitidos/VERB por/ADP uma/DET porta/NOUN ,/PUNCT por/ADP exemplo/NOUN ,/PUNCT sÃ£o/AUX eliminados/VERB por/ADP R$/SYM 150/NUM ./PUNCT\n",
      "\n",
      "Entrada: Que foi heranÃ§a do de o PT , que nos deixou esse rombo , disse Doria .\n",
      "Saida: Que/PRON foi/AUX heranÃ§a/NOUN do/None de/ADP o/DET PT/PROPN ,/PUNCT que/PRON nos/PRON deixou/VERB esse/DET rombo/NOUN ,/PUNCT disse/VERB Doria/PROPN ./PUNCT\n",
      "\n",
      "Entrada: O departamento mÃ©dico santista tambÃ©m jÃ¡ colocou o atleta para ter o acompanhamento de uma psicÃ³loga .\n",
      "Saida: O/DET departamento/NOUN mÃ©dico/ADJ santista/ADJ tambÃ©m/ADV jÃ¡/ADV colocou/VERB o/DET atleta/NOUN para/ADP ter/VERB o/DET acompanhamento/NOUN de/ADP uma/DET psicÃ³loga/NOUN ./PUNCT\n",
      "\n",
      "Entrada: Por que Ã© mais difÃ­cil para as mulheres lutar contra alcoolismo e dependÃªncia Ã s a as drogas ?\n",
      "Saida: Por/ADP que/PRON Ã©/AUX mais/ADV difÃ­cil/ADJ para/ADP as/DET mulheres/NOUN lutar/VERB contra/ADP alcoolismo/NOUN e/CCONJ dependÃªncia/NOUN Ã s/None a/ADP as/DET drogas/NOUN ?/PUNCT\n",
      "\n",
      "Entrada: Outra etiqueta das de as mais clÃ¡ssicas do de o calendÃ¡rio , a Bottega Venetta construiu com base nas em as pintas de leopardo a imagem de amazona moderna .\n",
      "Saida: Outra/DET etiqueta/NOUN das/None de/ADP as/PRON mais/ADV clÃ¡ssicas/ADJ do/None de/ADP o/DET calendÃ¡rio/NOUN ,/PUNCT a/DET Bottega/PROPN Venetta/PROPN construiu/VERB com/ADP base/NOUN nas/None em/ADP as/DET pintas/NOUN de/ADP leopardo/NOUN a/DET imagem/NOUN de/ADP amazona/NOUN moderna/ADJ ./PUNCT\n",
      "\n",
      "Entrada: NÃ£o Ã© biodegradÃ¡vel e , para produzÃ­-lo produzir lo , Ã© necessÃ¡rio expandir ainda mais a agricultura da de a cana .\n",
      "Saida: NÃ£o/ADV Ã©/AUX biodegradÃ¡vel/ADJ e/CCONJ ,/PUNCT para/ADP produzÃ­-lo/None produzir/VERB lo/PRON ,/PUNCT Ã©/AUX necessÃ¡rio/ADJ expandir/VERB ainda/ADV mais/ADV a/DET agricultura/NOUN da/None de/ADP a/DET cana/NOUN ./PUNCT\n",
      "\n",
      "Entrada: \" Temos seguranÃ§a de que ( ... ) sÃ£o Ã­ntegros \" , diz o texto .\n",
      "Saida: \"/PUNCT Temos/VERB seguranÃ§a/NOUN de/ADP que/SCONJ (/PUNCT .../PUNCT )/PUNCT sÃ£o/AUX Ã­ntegros/ADJ \"/PUNCT ,/PUNCT diz/VERB o/DET texto/NOUN ./PUNCT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "head = \"Atuando como linguista e sem efetuar correÃ§Ãµes ou alteraÃ§Ãµes no texto, faÃ§a a anÃ¡lise morfossintÃ¡tica das frases seguindo a anotaÃ§Ã£o UD (Universal Dependencies):\\n\\n\"\n",
    "promptStart = head\n",
    "for review in reviews[:10]:\n",
    "    entrada = \"Entrada: \"\n",
    "    saida  = \"Saida: \"\n",
    "    for token in review:\n",
    "        if not token[2]:\n",
    "            token[2] = 'None'\n",
    "        entrada += token[0] + ' '\n",
    "        saida += token[0] + '/' + token[2] + ' '\n",
    "    entrada = entrada.strip()\n",
    "    saida = saida.strip()\n",
    "    promptStart += entrada + \"\\n\" + saida + \"\\n\\n\"\n",
    "print(promptStart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7962653-1017-4a76-a87f-5e69fb7171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|âââââââââââââââââââââââââââââââ                                                                                                                                          | 18/100 [21:15<1:35:51, 70.14s/it]"
     ]
    }
   ],
   "source": [
    "punctList = ['\"', ',', '.', '[', ']', '?','!']\n",
    "             #, '\"/PUNCT', ',/PUNCT', './PUNCT']\n",
    "#punctList = []\n",
    "for temp in range(0,10):\n",
    "    if temp:\n",
    "        temp = temp/10\n",
    "    print('temp',temp)\n",
    "    gold = []\n",
    "    pred = []\n",
    "    totalTries = []\n",
    "    errors = 0\n",
    "    for revID, review in enumerate(tqdm(reviews[20:30])):\n",
    "        prompt = promptStart\n",
    "        entrada = \"Entrada: \"\n",
    "        saida  = \"Saida: \"\n",
    "        goldSent = []\n",
    "        goldTokens = []\n",
    "        for token in review:\n",
    "            if not token[2]:\n",
    "                token[2] = 'None'\n",
    "            #print(token)\n",
    "            if token[0] not in punctList:\n",
    "                gold.append(token[2])\n",
    "                goldSent.append(token[2])\n",
    "                goldTokens.append(token[0])\n",
    "            entrada += token[0] + ' '\n",
    "            saida += token[0] + '/' + token[2] + ' '\n",
    "        entrada = entrada.strip()\n",
    "        saida = saida.strip()\n",
    "        prompt += entrada + \"\\nSaÃ­da: \"\n",
    "        #print(prompt)\n",
    "        #sdad\n",
    "        fileName = f'resultados/data/gpt_t{temp}_{revID}.json'\n",
    "        if os.path.isfile(fileName):\n",
    "            with open(fileName) as file:\n",
    "                tagsPred = json.load(file)\n",
    "        else:            \n",
    "            tries = 0\n",
    "            while True:\n",
    "                if tries > 10:\n",
    "                    errors += 1\n",
    "                    totalTries.append(tries)\n",
    "                    with open(f'resultados/data/gpt_test_erro_t{temp}_{revID}.log', 'w') as f:\n",
    "                        f.write(prompt)\n",
    "                        f.write('======')\n",
    "                        f.write(answer+'\\n\\n')\n",
    "                        f.write('gold'+ str(goldSent) + str(len(goldSent))+'\\n')\n",
    "                        f.write(str(goldTokens)+'\\n')\n",
    "                        f.write('pred' + str(tagsSent) + str(len(tagsSent))+'\\n')\n",
    "                        f.write(str(tokens)+'\\n')\n",
    "                        f.write('\\n\\n')\n",
    "                    tagsPred = ['None'] * len(goldSent)\n",
    "                    #dsdaasd\n",
    "                    break\n",
    "                answer = askLLM(prompt, temperature=temp)\n",
    "                tries += 1\n",
    "                try:\n",
    "                    tokens = [token for token in answer.split(' ') if token[0] not in punctList]\n",
    "                    tokensSent = []\n",
    "                    tagsSent = []\n",
    "                    for token in tokens:\n",
    "                        if '/' in token:\n",
    "                            tagsSent.append(token.split('/')[1])\n",
    "                            tokensSent.append(token.split('/')[0])\n",
    "                        else:\n",
    "                            tagsSent.append('None')\n",
    "                            tokensSent.append(token)\n",
    "                except Exception as error:\n",
    "                    continue\n",
    "                idx = 0\n",
    "                erroIdx = 0\n",
    "                tagsPred = []\n",
    "                for token, tag in zip(goldTokens, goldSent):\n",
    "                    if idx >= len(tokensSent):\n",
    "                        tagsPred.append('None')\n",
    "                        erroIdx += 1\n",
    "                        continue           \n",
    "                    if tokensSent[idx].lower() == token.lower():\n",
    "                        tagsPred.append(tagsSent[idx])\n",
    "                    else:\n",
    "                        if idx == 0:\n",
    "                            while (tokensSent[idx].lower() != token.lower() and idx < 5):\n",
    "                                idx += 1\n",
    "                                if idx >= len(tokensSent):\n",
    "                                    idx -= 1\n",
    "                                    break\n",
    "                        if tokensSent[idx].lower() == token.lower():\n",
    "                            tagsPred.append(tagsSent[idx])\n",
    "                        else:\n",
    "                            tagsPred.append('None')\n",
    "                            erroIdx += 1\n",
    "                            continue\n",
    "                    idx += 1\n",
    "                if len(tagsPred) == len(goldSent) and erroIdx < 10:\n",
    "                    totalTries.append(tries)\n",
    "                    break\n",
    "        with open(fileName, 'w') as f:\n",
    "            json.dump(tagsPred, f)\n",
    "        pred += tagsPred\n",
    "        #print('=======')\n",
    "    print('gold',len(gold),'pred',len(pred),'errors',errors)\n",
    "    with open(f'resultados/gpt_test_t{temp}.json', 'w') as f:\n",
    "        json.dump(pred, f)\n",
    "    #with open(f'resultados/gold.json', 'w') as f:\n",
    "    #    json.dump(gold, f)\n",
    "    print(metrics.classification_report(gold, pred))\n",
    "    print(\"\\n\\n\\n\\n######\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac168b5-1c51-49e2-beb9-7da39a35c919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a484d-a093-4808-83e3-9e54c4a97b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf119e-02e2-4dfe-ab47-ac0c11c1cd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edcf43-30bf-4c10-9562-cbc4dcce3a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8f519-f10a-4730-a33d-b4f3441aaf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctList = ['\"', ',', '.', '[', ']', '?','!']\n",
    "             #, '\"/PUNCT', ',/PUNCT', './PUNCT']\n",
    "#punctList = []\n",
    "for temp in range(6,10):\n",
    "    temp = temp/10\n",
    "    print('temp',temp)\n",
    "    gold = []\n",
    "    pred = []\n",
    "    totalTries = []\n",
    "    errors = 0\n",
    "    for revID, review in enumerate(tqdm(reviews[20:])):\n",
    "        prompt = promptStart\n",
    "        entrada = \"Entrada: \"\n",
    "        saida  = \"Saida: \"\n",
    "        goldSent = []\n",
    "        goldTokens = []\n",
    "        for token in review:\n",
    "            if not token[2]:\n",
    "                token[2] = 'None'\n",
    "            #print(token)\n",
    "            if token[0] not in punctList:\n",
    "                gold.append(token[2])\n",
    "                goldSent.append(token[2])\n",
    "                goldTokens.append(token[0])\n",
    "            entrada += token[0] + ' '\n",
    "            saida += token[0] + '/' + token[2] + ' '\n",
    "        entrada = entrada.strip()\n",
    "        saida = saida.strip()\n",
    "        prompt += entrada + \"\\nSaÃ­da: \"\n",
    "        #print(prompt)\n",
    "        #sdad\n",
    "        fileName = f'resultados/data/gpt_t{temp}_{revID}.json'\n",
    "        if os.path.isfile(fileName):\n",
    "            with open(fileName) as file:\n",
    "                tagsPred = json.load(file)\n",
    "        else:            \n",
    "            tries = 0\n",
    "            while True:\n",
    "                if tries > 10:\n",
    "                    errors += 1\n",
    "                    totalTries.append(tries)\n",
    "                    with open(f'resultados/data/gpt_erro_t{temp}_{revID}.log', 'w') as f:\n",
    "                        f.write(prompt)\n",
    "                        f.write('======')\n",
    "                        f.write(answer+'\\n\\n')\n",
    "                        f.write('gold'+ str(goldSent) + str(len(goldSent))+'\\n')\n",
    "                        f.write(str(goldTokens)+'\\n')\n",
    "                        f.write('pred' + str(tagsSent) + str(len(tagsSent))+'\\n')\n",
    "                        f.write(str(tokens)+'\\n')\n",
    "                        f.write('\\n\\n')\n",
    "                    tagsPred = ['None'] * len(goldSent)\n",
    "                    #dsdaasd\n",
    "                    break\n",
    "                answer = askLLM(prompt, temperature=temp)\n",
    "                tries += 1\n",
    "                try:\n",
    "                    tokens = [token for token in answer.split(' ') if token[0] not in punctList]\n",
    "                    tokensSent = []\n",
    "                    tagsSent = []\n",
    "                    for token in tokens:\n",
    "                        if '/' in token:\n",
    "                            tagsSent.append(token.split('/')[1])\n",
    "                            tokensSent.append(token.split('/')[0])\n",
    "                        else:\n",
    "                            tagsSent.append('None')\n",
    "                            tokensSent.append(token)\n",
    "                except Exception as error:\n",
    "                    continue\n",
    "                idx = 0\n",
    "                erroIdx = 0\n",
    "                tagsPred = []\n",
    "                for token, tag in zip(goldTokens, goldSent):\n",
    "                    if idx >= len(tokensSent):\n",
    "                        tagsPred.append('None')\n",
    "                        erroIdx += 1\n",
    "                        continue           \n",
    "                    if tokensSent[idx].lower() == token.lower():\n",
    "                        tagsPred.append(tagsSent[idx])\n",
    "                    else:\n",
    "                        if idx == 0:\n",
    "                            while (tokensSent[idx].lower() != token.lower() and idx < 5):\n",
    "                                idx += 1\n",
    "                                if idx >= len(tokensSent):\n",
    "                                    idx -= 1\n",
    "                                    break\n",
    "                        if tokensSent[idx].lower() == token.lower():\n",
    "                            tagsPred.append(tagsSent[idx])\n",
    "                        else:\n",
    "                            tagsPred.append('None')\n",
    "                            erroIdx += 1\n",
    "                            continue\n",
    "                    idx += 1\n",
    "                if len(tagsPred) == len(goldSent) and erroIdx < 10:\n",
    "                    totalTries.append(tries)\n",
    "                    break\n",
    "        with open(fileName, 'w') as f:\n",
    "            json.dump(tagsPred, f)\n",
    "        pred += tagsPred\n",
    "        #print('=======')\n",
    "    print('gold',len(gold),'pred',len(pred),'errors',errors)\n",
    "    with open(f'resultados/gpt_temp{temp}.json', 'w') as f:\n",
    "        json.dump(pred, f)\n",
    "    #with open(f'resultados/gold.json', 'w') as f:\n",
    "    #    json.dump(gold, f)\n",
    "    print(metrics.classification_report(gold, pred))\n",
    "    print(\"\\n\\n\\n\\n######\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a04e498f-2673-468a-8be3-ec18e1cdb914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "391b0615-feb5-4b76-ae2e-23183fc5c8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ede869c-fc10-4758-a7eb-6faf64fdf162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebf732-a1ea-4afd-8099-e1ab5284983b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc4b58-39ec-42ab-bd51-e536dfec06f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130298b-0ac0-49f5-bd44-db2b6f857dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a23520-2400-4307-a8c2-f7b96b3a5666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729d970-412d-4453-b8c8-87beb99a5ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6f6b1fd-59f5-44a3-82a6-d20dcbe73970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['VocÃª', None, 'PRON', 'dislocated'],\n",
       "  [',', None, 'PUNCT', 'punct'],\n",
       "  ['por', None, 'ADP', 'case'],\n",
       "  ['exemplo', None, 'NOUN', 'obl'],\n",
       "  [',', None, 'PUNCT', 'punct'],\n",
       "  ['vocÃª', None, 'PRON', 'nsubj'],\n",
       "  ['brilha', None, 'VERB', 'root'],\n",
       "  ['um', None, 'DET', 'det'],\n",
       "  ['pouco', None, 'NOUN', 'obl'],\n",
       "  ['.', None, 'PUNCT', 'punct']],\n",
       " [['A', None, 'DET', 'det'],\n",
       "  ['gente', None, 'NOUN', 'nsubj'],\n",
       "  ['vai', None, 'AUX', 'aux'],\n",
       "  ['comprar', None, 'VERB', 'root'],\n",
       "  ['pÃ£o', None, 'NOUN', 'obj'],\n",
       "  ['e', None, 'CCONJ', 'cc'],\n",
       "  ['fica', None, 'VERB', 'conj'],\n",
       "  ['ouvindo', None, 'VERB', 'xcomp'],\n",
       "  ['muita', None, 'ADJ', 'amod'],\n",
       "  ['coisinha', None, 'NOUN', 'obj'],\n",
       "  ['.', None, 'PUNCT', 'punct']]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[20:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c79741b-1eb0-43f2-b2a4-fd68aa4b4bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: A anÃ¡lise morfossintÃ¡tica Ã© um processo de anÃ¡lise linguÃ­stica que examina a estrutura das sentenÃ§as, identificando suas partes constituintes, como substantivos, verbos, adjetivos e preposiÃ§Ãµes, e suas relaÃ§Ãµes sintÃ¡ticas entre si. A Universal Dependencies (UD) Ã© um conjunto de regras de anotaÃ§Ã£o que permite a representaÃ§Ã£o dessas informaÃ§Ãµes em uma forma padronizada.\n",
      "\n",
      "Aqui estÃ£o as anÃ¡lises morfossintÃ¡ticas das frases fornecidas, utilizando a notaÃ§Ã£o UD:\n",
      "\n",
      "Entrada: O CapitÃ£o AmÃ©rica tambÃ©m bajulou o tucano .\n",
      "Saida: O/DET CapitÃ£o/PROPN AmÃ©rica/PROPN tambÃ©m/ADV bajulou/VERB o/DET tucano/NOUN ./PUNCT\n",
      "\n",
      "Entrada: A Odebrecht pagou 300 % a mais pelo o direito de explorar o aeroporto do de o GaleÃ£o .\n",
      "Saida: A/DET Odebrecht/PROPN pagou/VERB 300/NUM %/SYM a/ADP mais/ADV pelo/None por/ADP o/DET direito/NOUN de/ADP explorar/VERB o/DET aeroporto/NOUN do/None de/ADP o/DET GaleÃ£o/PROPN ./PUNCT\n",
      "\n",
      "Entrada: No Em o comeÃ§o do de o sÃ©culo , a JBS/Friboi chegava ao a o grupo das de as 400 maiores .\n",
      "Saida: No/None Em/ADP o/DET comeÃ§o/NOUN do/None de/ADP o/DET sÃ©culo/NOUN ,/PUNCT a/DET JBS/Friboi/PROPN chegava/VERB ao/None a/ADP o/DET grupo/NOUN das/None de/ADP as/DET 400/NUM maiores/ADJ ./PUNCT\n",
      "\n",
      "Entrada: Os sons indesejÃ¡veis emitidos por uma porta , por exemplo , sÃ£o eliminados por R$ 150 .\n",
      "Saida: Os/DET\n",
      "Resposta: A gente/DET vai/VERB comprar/VERB pÃ£o/NOUN e/CCONJ fica/VERB ouvindo/AUX muita/ADV coisinha/NOUN ./PUNCT\n"
     ]
    }
   ],
   "source": [
    "gold = []\n",
    "pred = []\n",
    "for review in reviews[20:22]:\n",
    "    prompt = promptStart\n",
    "    entrada = \"Entrada: \"\n",
    "    saida  = \"Saida: \"\n",
    "    for token in review:\n",
    "        if not token[2]:\n",
    "            token[2] = 'None'\n",
    "        gold.append(token[2])\n",
    "        entrada += token[0] + ' '\n",
    "        saida += token[0] + '/' + token[2] + ' '\n",
    "    entrada = entrada.strip()\n",
    "    saida = saida.strip()\n",
    "    prompt += entrada + \"\\nSaÃ­da: \"\n",
    "\n",
    "    answer = model.generate(prompt)\n",
    "    print(f\"Resposta: {answer}\")\n",
    "    print('=======')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ade0be5f-e388-41cd-935c-35ba2a3a2545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON',\n",
       " 'PUNCT',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'PRON',\n",
       " 'VERB',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'PUNCT',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'AUX',\n",
       " 'VERB',\n",
       " 'NOUN',\n",
       " 'CCONJ',\n",
       " 'VERB',\n",
       " 'VERB',\n",
       " 'ADJ',\n",
       " 'NOUN',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237609dc-17e3-463e-9c0d-5539f5b81571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0b1719-a92b-4f50-9f2a-13efa35f231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maritaca_key =  os.getenv('MARITACA_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d2ec6d-afd4-4fe8-a860-95b3cdc946bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: 25 + 27 = 52\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0a115ed-1641-40ce-8b81-2e89ca9159b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Mr./NNP Bean/NNP loves/VBZ giant/JJ bees/NNS ./.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: Ms. Haag plays Elianti .\n",
    "Output: Ms./NNP Haag/NNP plays/VBZ Elianti/NNP ./.\n",
    "\n",
    "Input: Mr. Bean loves giant bees.\n",
    "Output: \"\"\"\n",
    "answer = model.generate(prompt)\n",
    "print(f\"Resposta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5fcaac-1536-41f1-88e1-7de72207dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Mr./NNP Bean/NNP adora/VBZ abelhas/NNP gigantes/ADJ ./.\n",
      "\n",
      "Input: Ms. Bean adora abelhas gigantes.\n",
      "Output: Ms./NNP Bean/NNP adora/VBZ abelhas/NNP gigantes/ADJ ./.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: Ms. Haag plays Elianti .\n",
    "Output: Ms./NNP Haag/NNP plays/VBZ Elianti/NNP ./.\n",
    "\n",
    "Input: Mr. Bean adora abelhas gigantes.\n",
    "Output: \"\"\"\n",
    "answer = model.generate(prompt)\n",
    "print(f\"Resposta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930e24a9-d7e2-4eff-93d7-34f55b549252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Uma/DT bela/JJ casa/NN amarela/JJ .\n",
      "\n",
      "The second input you provided is a sentence in Portuguese, which is not a language I am capable of understanding or generating. If you have any questions or requests, please let me know and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Input: Ms. Haag plays Elianti .\n",
    "Output: Ms./NNP Haag/NNP plays/VBZ Elianti/NNP ./.\n",
    "\n",
    "Input: Uma bela casa amarela.\n",
    "Output: \"\"\"\n",
    "answer = model.generate(prompt)\n",
    "print(f\"Resposta: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51d919-cfc2-4f2f-b5a8-ee59ae5162ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
